# 算法岗面试编程题总结
## 1. 手写一个Kmeans

总体结构分三部分：
1. 初始化质心
2. 计算最小距离，分配聚类
3. 如果和result中存储的上一轮聚类不同，则更新result
4. 重新分配质心
   1. 找出所有被分配到同一聚类的样本
   2. 所有同一聚类样本的平均值作为质心

```
## Kmeans by JeffWang
def kmeans(dataSet, k):
    m = shape(dataSet)[0]
    result = mat(zeros(m, 2))
    # 1. 初始化质心
    centers = randCent(dataSet, k)
    flag = True
    while flag:
        for i in range(m):
            mindist = inf
            # 2. 对于每一条样本，计算到质心的最小距离，并分配类别。
            for j in range(k):
                dist = distEclud(centers[j,:], dataSet[i,:])
                if dist<mindist:
                    mindist = dist
                    minId = j
            # 3. 如果result中存储的id和minid不一样，更新result
            if minId != result[i, 0]:
                result[i, 0] = minId
                flag = True
        # 4. 重新分配质心
        for cent in range(k):
            # 4.1 找出所有被分配到同一聚类的样本
            samecluster = dataSet[nonzero(centers[:, 0].A==i)[0]]
            # 4.2 所有同一聚类样本的平均值为质心
            centers[i,:] = mean(samecluster)
        return result, centers
```
初始化质心：
```
# 随机生成初始质心
def randCent(dataSet, k):
    n = shape(dataSet)[1]
    centroids = mat(zeros(k,n))
    for j in range(n):
        minJ = min(dataSet[:,j])
        rangeJ = float(max(array(dataSet)[:,j])-minJ)
        centroids[:,j] = minJ+rangeJ*random.rand(k,1)
    return centroids
```
计算欧式距离：
```
# 计算两个向量之间的距离
def distEclud(vecA, vecB):
    return sqrt(sum(power(vecA-vecB, 2)))
```

## 2. 使用tensorflow的Keras手写一个两层的MLP
```
# 7. 定义神经网络模型：2层MLP
# 输入层为784个节点
# 中间层为512个节点
# 输出层为10个节点
from keras.models import Sequential
from keras.layers import Dense,Dropout,Flatten

model = Sequential()
model.add(Flatten(input_shape = x_train.shape[1:]))
model.add(Dense(512, activation = 'relu'))
model.add(Dropout(0.2))
model.add(Dense(512, activation = 'relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation = 'softmax'))

# 8. 编译模型
model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

# 9. 训练模型
from keras.callbacks import ModelCheckpoint

checkpointer = ModelCheckpoint(filepath = 'mnist.model.best.hdf5',verbose=1, save_best_only=True)
hist = model.fit(x_train, y_train, batch_size=128, epochs=10,
          validation_split=0.2, callbacks=[checkpointer],
          verbose=1, shuffle=True)

# 10. 在测试集上测试准确率
model.load_weights('mnist.model.best.hdf5')

score = model.evaluate(x_test, y_test, verbose=0)
accuracy = 100 * score[1]
print('Test accuracy: %.4f%%' % accuracy)
```

## 3. 仅使用 numpy 写一个神经网络
```
def nn_model(X, Y):
    np.random.seed(3)
    # 定义输入层和输出层大小
    n_x = X.shape[0]
    n_y = Y.shape[0]

    # 定义隐藏层节点数
    n_h = 4

    # 初始化参数
    W1 = np.random.randn(n_h, n_x)*0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_h, n_x)*0.01
    b2 = np.zeros((n_y, 1))

    # 开始迭代
    for i in range(0, 10000):
        # 前向传播
        Z1 = np.dot(W1, X)+b1
        A1 = np.tanh(Z1)
        Z2 = np.dot(W2, X)+b2
        A2 = 1/(1+np.e^(-Z2))

        # 计算损失函数，交叉熵
        logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), 1-Y)
        cost = -1/Y.shape[1] * np.sum(logprobs)
        cost = np.squeeze(cost)

        # 反向传播
        m = X.shape[1]
        dZ2 = A2-Y
        dW2 = 1/m * np.dot(dZ2, A1.T)
        db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)
        dZ1 = np.dot(W2.T, dZ2)*(1-np.power(A1, 2))
        dW1 = 1/m * np.dot(dZ1, X.T)
        db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)

        # 更新权重
        learning_rate = 1.2
        W1 -= dW1*learning_rate
        b1 -= db1*learning_rate
        W2 -= dW2*learning_rate
        b2 -= db2*learning_rate

    return W1, b1, W2, b2
```

## 4. 多层感知机表示异或逻辑时，最少需要几个隐藏层？
$$Z_1 = relu(X+Y-1)$$
$$Z_2 = relu(-X-Y+1)$$
$$Z = -Z_1-Z_2+1$$

## 5. 如果只是用一个隐藏层，需要多少隐藏节点能够实现包含n元输入的任意布尔函数？
$$2^{(n-1)}$$

## 6. 考虑多隐藏层的情况，实现包含n元输入的任意布尔函数最少需要多少个网络节点和网络层？
网络层数：
$$2log_2N$$
隐藏节点数：
$$O(3(n-1))$$