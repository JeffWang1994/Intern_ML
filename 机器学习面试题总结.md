# 机器学习面试题总结
https://blog.csdn.net/woaidapaopao/article/details/77806273
## 1. 请简要介绍一下SVM
SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。<br>
线性可分支持向量机、线性支持向量机及非线性支持向量机。当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；<br>
当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；<br>
当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

## 2. 请简要介绍一下Tensorflow计算图
计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。<br>
Tensorflow分为两个部分：<br>
构造部分：包含计算流图。<br>
执行部分：通过session来执行图中的计算。<br>

## 3. GBDT和XGBoost的区别是什么？
XGBoost的优点：
    1. 损失函数是泰勒展式二项逼近，而不是GBDT的一阶导数。
    2. 对树的结构进行正则化约束，防止模型过度复杂，降低了过拟合的可能性。
    3. 节点分裂的方式不同，GBDT是基尼系数，XGBoost是经过优化推导后的。
   
## 4. k-means或kNN, 我们是用欧式距离来计算最近的邻居之间的距离，而不是用曼哈顿距离，为什么？
曼哈顿距离——街区距离：
$$d_{12}=|x_1-x_2|+|y_1-y_2|$$
曼哈顿距离只计算水平或垂直距离，有维度的限制。欧氏距离可用于任何空间的距离计算。

## 5. LR的推导公式，正则化，maxent模型关系，LR为什么比线性回归好。
https://blog.csdn.net/sinat_35512245/article/details/54881672

## 6. 过拟合怎么办？
加数据！降低复杂度；正则化；集成学习

## 7. LR和SVM的联系与区别？
LR是参数模型，SVM是非参数模型。<br>
LR的目标函数是logistical loss，SVM的目标函数为hinge loss。<br>

## 8.为什么XGBoost要用泰勒展开，优势是什么？
二阶偏导有利于梯度下降更快更准。

## 9.Xgboost是如何寻找最优特征的？是有放回还是无放回？
XGBoost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性.。XGBoost利用梯度优化模型算法, 样本是不放回的。但XGBoost支持子采样, 也就是每轮计算可以不使用全部样本。

## 10.判别式模型和生成式模型？
判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。<br>
生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。<br>
由生成模型可以得到判别模型，但由判别模型得不到生成模型。<br>
常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场<br>
常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机<br>

## 11.CNN最成功的应用是CV，那么为什么NLP和Speech还有AlphaGo中都用到CNN？
https://zhuanlan.zhihu.com/p/25005808

## 12. Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的权重是w1,w2,w3...，请写出最终的决策公式。
http://www.360doc.com/content/14/1109/12/20290918_423780183.shtml

## 13. 为什么朴素贝叶斯如此“朴素”？
因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。

## 14. 机器学习中，为何要经常对数据做归一化？
线性归一化；标准差归一化；非线性归一化
http://www.cnblogs.com/LBSer/p/4440590.html

## 15. new和malloc的区别？
melloc为memory allocation， 动态内存分配。
https://www.cnblogs.com/fly1988happy/archive/2012/04/26/2470542.html

## 16. Hash冲突及解决方法？
关键字值不同的元素可能会映象到哈希表的同一地址上就会发生哈希冲突。解决办法： <br>
1）开放定址法：当冲突发生时，使用某种探查(亦称探测)技术在散列表中形成一个探查(测)序列。沿此序列逐个单元地查找，直到找到给定 的关键字，或者碰到一个开放的地址(即该地址单元为空)为止（若要插入，在探查到开放的地址，则可将待插入的新结点存人该地址单元）。查找时探查到开放的 地址则表明表中无待查的关键字，即查找失败。 <br>
2） 再哈希法：同时构造多个不同的哈希函数。 <br>
3）链地址法：将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。 <br>
4）建立公共溢出区：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。<br>

## 17.梯度消失和梯度膨胀？
（1）梯度消失： <br>
根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0。 <br>
可以采用ReLU激活函数有效的解决梯度消失的情况。 <br>
（2）梯度膨胀： <br>
根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。<br>
可以通过激活函数来解决。<br>

## 18. 红黑树
https://blog.csdn.net/v_july_v/article/details/6105630

## 19. 卷积？
对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。<br>
非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。

## 20. 什么是池化层？
池化，简言之，即取区域平均或最大。

## 21. 哪些机器学习不需要做归一化处理？
概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、GBDT、XGBoost、SVM、LR、KNN、KMeans之类的最优化问题就需要归一化。

## 牛顿法和梯度下降法有什么不同？
梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。<br>
批量梯度下降---最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。<br>
随机梯度下降---最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。<br>
牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。<br>
从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）<br>
根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。<br>
拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。<br>
共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。<br>

## 介绍一下CNN发展历程
https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2651986617&idx=1&sn=fddebd0f2968d66b7f424d6a435c84af&scene=0#wechat_redirect

## HashMap和HashTable
1. HashTable不允许null值(key和value都不可以),HashMap允许null值(key和value都可以)。
2. HashTable有一个contains(Object value)，功能和containsValue(Object value)功能一样。
3. HashTable使用Enumeration，HashMap使用Iterator。
4. 哈希值的使用不同，HashTable直接使用对象的hashCode，代码是这样的：
    int hash = key.hashCode();
    int index = (hash & 0x7FFFFFFF) % tab.length;
   而HashMap重新计算hash值，而且用与代替求模：
    int hash = hash(k);
    int i = indexFor(hash, table.length);

## 随机森林如何处理缺失值？
方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。<br>
方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似1缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩2。<br>

## 随机森林如何评估特征重要性？
衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：<br>
1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。<br>
2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。<br>

## KMeans++初始类簇中心点的选取
K-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。<br>
1.从输入的数据点集合中随机选择一个点作为第一个聚类中心 <br>
2.对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) <br>
3.选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 
4.重复2和3直到k个聚类中心被选出来 <br>
5.利用这k个初始的聚类中心来运行标准的k-means算法<br>

## 如何进行特征选择?
1. 去除方差较小的特征。
2. 正则化

## 有哪些特征选择的工程方法？
数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已<br>
1.计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了； <br>
2.构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征； <br>
3.通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验*； <br>
4.训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型； <br>
5.通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。 <br>
6.通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。<br>
