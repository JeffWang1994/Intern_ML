# 机器学习基础
## 1. 常见的距离有哪些？
1. 欧式距离
2. 余弦相似度
3. Jaccord距离
4. 曼哈顿距离

## 2. 如何降低过拟合和欠拟合
过拟合：
1. 获得更多训练数据，甚至可以生成更多训练数据
2. 降低模型复杂度，减少网络层数，神经元个数。对于决策树，进行剪枝，降低树的深度。
3. 正则化，L1和L2正则化
4. 使用集成学习。

欠拟合：
1. 添加新的特征，因子分解机，GBDT，Deep-Cross等等
2. 增加模型复杂度，增加网络层数，神经元个数。
3. 减小正则化系数
   
## 3. 有监督学习的损失函数有哪些？
对于分类问题：
1. 0-1损失函数
   $$L_{0-1}(f, y) = 1_{fy\leqslant0}$$
2. Hinge损失函数
   $$L_{hinge}(f, y) = max \{0, 1-fy\} $$
3. Logistic损失函数
   $$L_{logistic}(f, y) = log(1+exp(-fy))$$
4. 交叉熵
   $$L_{cross entropy}(f, y) = -log(\frac{1+fy}{2})$$

对于回归问题：
1. 均方误差
   $$L_{square}(f, y) = (f-y)^2$$
2. 绝对损失函数
   $$L_{absolute}(f, y) = |f-y|$$
3. Huber损失函数
   $$L_{Huber}(f, y) = $$

## 4. 机器学习中哪些问题是凸优化问题？
逻辑回归是凸优化问题。

## 5. L1正则化和L2正则化
L2正则化：加上权重的平方和
$$L = E_{in}+\lambda \sum_j{w_j^2}$$
L1正则化：加上权重参数的绝对值
$$L = E_{in}+\lambda \sum_j{|w_j|}$$

L2正则化的推导：
可以形成一个新的损失函数：
$$E_{aug} = E_{in} + \frac{\lambda}{2}w^2$$